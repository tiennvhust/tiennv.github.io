<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Real-time sonification algorithm</title>
    <style>
      video {
        width: 100%;
        height: auto;
      }

      /* Style for video container */
      .video-container {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
        align-items: flex-start;
      }
      
      /* Style for video box */
      .video-box {
        flex-basis: 48%;
        /*margin-bottom: 1rem;*/
        text-align: justify;
      }
    </style>
    <link rel="stylesheet" href="style/textstyle.css">
  </head>
  <body>
      <div class="description">
        <h1>Real-time, AI-assisted Sonification Algorithm for Neonatal EEG</h1>
        <h3>DESCRIPTION</h3>
        <p>Neonatal seizures are a critical problem globally, which can lead to long-term developmental and neurological disabilities or even death. Early detection of seizures is crucial for preventing these outcomes by enabling timely intervention. However, clinical detection of neonatal seizures is challenging due to the lack of physical symptoms and limited access to experts in EEG analysis. Artificial Intelligence (AI) has emerged as a popular tool to assist medical professionals in interpreting EEG signals and detecting seizures. A novel method of AI-assisted EEG sonification was introduced to compensate for the lack of explainability in AI’s decisions. This method uses sound to detect seizures intuitively while exploiting AI’s effectiveness as an attention mechanism. Besides low power consumption, the real-time operating capability is also essential for adapting this algorithm in clinical settings, which is unattainable with the offline processing approach used in previous studies. This study presents a scalable and real-time adaptation of this algorithm with an ultra-low power implementation. This application provides continuous audio output for medical workers, allowing for immediate access to audio analysis of the EEG signals. An on-chip ultra-low power neural network accelerator enables the implementation to scale up the number of monitored EEG channels. The real-time algorithm has an average power consumption of 13 milliwatts, allowing it to operate for more than eleven days on a mobile phone battery with a capacity of 3500 mAh.</p>
        <ol>
            <b><li>Original AI-assited Sonification Algorithm</li></b>
            <div class="video-container">
              <div class="video-box">
                <p>A novel method of AI-driven spatial neonatal EEG sonification has been presented in [1]. It utilizes the Phase Vocoder (PV) algorithm with an AI-guided time-compression factor as an attention mechanism. The AI probabilities are used to modulate the audio playback speed, maintaining focus on the EEG regions with a high probability of seizures, while allowing quick audio rendering over non-seizure EEG segments. This method combines the accuracy and speed of AI algorithms with the interpretability and intuitiveness of sonification, it enables a quick, accurate, and interpretable seizure detection method while requiring little to no training from medical workers.</p>
              </div>
              <div class="video-box">
                <img src="../images/original.gif" style="width:400px;">
              </div>
            </div>
            <b><li>Real-Time Adaptation</li></b>
            <div class="video-container">
              <div class="video-box">
                <p>This study presents a real-time adaptation of the AI-assisted EEG sonification algorithm and its implementation on an ultra-low power embedded system. A significant advantage of a real-time implementation is that the output audio is always available for listening. Medical workers can have access to real-time audio at any time without having to periodically restart the sonification algorithm, enabling seizure detection as soon as they occur. This ultra-low-power implementation is suitable for wearable devices, which can be widely adopted in clinical settings, including Intensive Care Units, for continuous neonatal EEG monitoring.</p>
              </div>
              <div class="video-box">
                <img src="../images/realtime.gif" style="width:400px;">
              </div>
            </div>
        </ol>
        <h3>Deep Learning Algorithm Quantization</h3>
        <p>Before the model can be deployed on the MAX78002’s deep neural network accelerator, its parameters must be quantized to 8-bit integers. In practice, there are two main techniques for quantization: Post Training Quantization (PTQ) and Quantization Aware Training (QAT). On the one hand, PTQ can be used to quantize a pre-trained model without any re-training requirements. QAT on the other hand immensely improves the post-quantization performance. The dataset used for quantization aware training and validation is a public dataset of neonatal EEG provided by the Helsinki University Hospital [2]. This dataset is split into a trainset of 39 neonates and a test set of 40 neonates, each dataset contains approximately the same percentage of seizures.</p>
        <p>Table I compares the performance of the original model and its 8-bits integer version against the test set. The results showed that the quantized model has a 0.8% drop in Area under the ROC Curve (AUC). However, by converting the model from 32-bit floating point into 8-bit integer the size of the model was reduced by 75% and the inference speed is significantly increased.</p>
        <table>
          <caption>TABLE I. 	POST QUANTIZATION PERFORMANCE</caption>
          <tr>
            <th>Model</th>
            <th>Bit-width</th>
            <th>Number of Parameters</th>
            <th>Size (bytes)</th>
            <th>AUC (%)</th>
          </tr>
          <tr>
            <td>Original</td>
            <td>32</td>
            <td>25,156</td>
            <td>102,152</td>
            <td>95.4</td>
          </tr>
          <tr>
            <td>Quantized</td>
            <td>8</td>
            <td>25,156</td>
            <td>25,156</td>
            <td>94.6</td>
          </tr>
        </table>
        <h3>REFERENCES</h3>
        <p>[1]&nbsp;S. Gomez-Quintana, A. O’Shea, A. Factor, E. Popovici, and A. Temko, “A method for AI assisted human interpretation of neonatal EEG,” Sci Rep, vol. 12, no. 1, p. 10932, 2022, doi: 10.1038/s41598-022-14894-4.</p>
      </div>
  </body>
</html>
